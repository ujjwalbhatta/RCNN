{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def load_model(model_path, num_classes, device):\n",
    "    \"\"\"\n",
    "    Load the trained RCNN model\n",
    "    \"\"\"\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    \"\"\"\n",
    "    Prepare the image for inference\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    image_tensor = F.to_tensor(image)\n",
    "    image_tensor = F.normalize(image_tensor, \n",
    "                             mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    return image, image_tensor\n",
    "\n",
    "def draw_predictions(image, boxes, labels, scores, idx_to_label, threshold=0.5, ignore_labels={\"DontCare\", \"Unknown\"}):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on the image\n",
    "    \"\"\"\n",
    "    colors = {\n",
    "        'Car': (255, 0, 0),      # Red\n",
    "        'Pedestrian': (0, 255, 0),  # Green\n",
    "        'Van': (0, 0, 255),      # Blue\n",
    "        'Cyclist': (255, 255, 0),  # Yellow\n",
    "        'Truck': (255, 0, 255),   # Magenta\n",
    "        'Misc': (0, 255, 255),    # Cyan\n",
    "        'Tram': (128, 0, 0),     # Dark Red\n",
    "        'Person_Sitting': (0, 128, 0),  # Dark Green\n",
    "        'DontCare': (128, 128, 128),  # Gray\n",
    "        'Unknown': (64, 64, 64)   # Dark Gray\n",
    "    }\n",
    "    \n",
    "    image_with_boxes = image.copy()\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > threshold:\n",
    "            class_name = idx_to_label[label]\n",
    "            \n",
    "            # Skip ignored labels\n",
    "            if class_name in ignore_labels:\n",
    "                continue\n",
    "            \n",
    "            # Convert box coordinates to integers\n",
    "            box = box.astype(int)\n",
    "            color = colors.get(class_name, (255, 255, 255))\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(image_with_boxes, \n",
    "                         (box[0], box[1]), \n",
    "                         (box[2], box[3]), \n",
    "                         color, \n",
    "                         2)\n",
    "            \n",
    "            # Add label and score\n",
    "            label_text = f'{class_name}: {score:.2f}'\n",
    "            cv2.putText(image_with_boxes, \n",
    "                       label_text, \n",
    "                       (box[0], box[1] - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.5, \n",
    "                       color, \n",
    "                       2)\n",
    "    \n",
    "    return image_with_boxes\n",
    "\n",
    "def test_single_image(model_path, image_path, label_to_idx, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Test the model on a single image\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create idx to label mapping\n",
    "    idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(model_path, len(label_to_idx), device)\n",
    "    \n",
    "    # Prepare image\n",
    "    original_image, image_tensor = prepare_image(image_path)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor.to(device)])\n",
    "    \n",
    "    # Get predictions\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "    labels = prediction[0]['labels'].cpu().numpy()\n",
    "    scores = prediction[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Draw predictions on image, ignoring specified labels\n",
    "    result_image = draw_predictions(original_image, \n",
    "                                  boxes, \n",
    "                                  labels, \n",
    "                                  scores, \n",
    "                                  idx_to_label, \n",
    "                                  confidence_threshold)\n",
    "    \n",
    "    # Convert back to BGR for OpenCV display/save\n",
    "    result_image = cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Save the result\n",
    "    output_path = 'prediction_result.jpg'\n",
    "    cv2.imwrite(output_path, result_image)\n",
    "    print(f\"Result saved to: {output_path}\")\n",
    "    \n",
    "    return boxes, labels, scores\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = '/Users/ujjwalbhatta/Downloads/ComputerVision-RCNN/final_rcnn_model.pth' \n",
    "    image_path = '/Users/ujjwalbhatta/Downloads/ComputerVision-RCNN/RCNN_Dataset/test/007301.png'\n",
    "    \n",
    "    label_to_idx = {\n",
    "        \"Car\": 1,\n",
    "        \"Pedestrian\": 2,\n",
    "        \"Van\": 3,\n",
    "        \"Cyclist\": 4,\n",
    "        \"Truck\": 5,\n",
    "        \"Misc\": 6,\n",
    "        \"Tram\": 7,\n",
    "        \"Person_Sitting\": 8,\n",
    "        \"DontCare\": 9,\n",
    "        \"Unknown\": 10\n",
    "    }\n",
    "    \n",
    "    boxes, labels, scores = test_single_image(model_path, image_path, label_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def load_model(model_path, num_classes, device):\n",
    "    \"\"\"\n",
    "    Load the trained RCNN model\n",
    "    \"\"\"\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def prepare_frame(frame):\n",
    "    \"\"\"\n",
    "    Prepare the frame for inference\n",
    "    \"\"\"\n",
    "    # Convert from BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert to tensor and normalize\n",
    "    frame_tensor = F.to_tensor(frame)\n",
    "    frame_tensor = F.normalize(frame_tensor, \n",
    "                             mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    return frame, frame_tensor\n",
    "\n",
    "def process_video(model_path, video_path, label_to_idx, confidence_threshold=0.5, output_path='prediction_result.mp4'):\n",
    "    \"\"\"\n",
    "    Process a video file and save the result with bounding boxes and labels\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create idx to label mapping\n",
    "    idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(model_path, len(label_to_idx), device)\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"Video properties - FPS: {fps}, Width: {width}, Height: {height}\")\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Process each frame\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"No more frames to read or unable to read the frame.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Processing frame: {frame_count}\")\n",
    "        original_frame, frame_tensor = prepare_frame(frame)\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            prediction = model([frame_tensor.to(device)])\n",
    "        \n",
    "        # Get predictions\n",
    "        boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "        labels = prediction[0]['labels'].cpu().numpy()\n",
    "        scores = prediction[0]['scores'].cpu().numpy()\n",
    "        \n",
    "        print(f\"Predictions - Boxes: {boxes.shape}, Labels: {labels.shape}, Scores: {scores.shape}\")\n",
    "\n",
    "        # Draw predictions on the frame\n",
    "        result_frame = draw_predictions(original_frame, \n",
    "                                       boxes, \n",
    "                                       labels, \n",
    "                                       scores, \n",
    "                                       idx_to_label, \n",
    "                                       confidence_threshold)\n",
    "        \n",
    "        # Write the frame to the output video\n",
    "        out.write(result_frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    # Release the resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Result saved to: {output_path}\")\n",
    "\n",
    "def draw_predictions(image, boxes, labels, scores, idx_to_label, threshold=0.5, ignore_labels={\"DontCare\", \"Unknown\"}):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on the image\n",
    "    \"\"\"\n",
    "    colors = {\n",
    "        'Car': (255, 0, 0),      # Red\n",
    "        'Pedestrian': (0, 255, 0),  # Green\n",
    "        'Van': (0, 0, 255),      # Blue\n",
    "        'Cyclist': (255, 255, 0),  # Yellow\n",
    "        'Truck': (255, 0, 255),   # Magenta\n",
    "        'Misc': (0, 255, 255),    # Cyan\n",
    "        'Tram': (128, 0, 0),     # Dark Red\n",
    "        'Person_Sitting': (0, 128, 0),  # Dark Green\n",
    "        'DontCare': (128, 128, 128),  # Gray\n",
    "        'Unknown': (64, 64, 64)   # Dark Gray\n",
    "    }\n",
    "    \n",
    "    image_with_boxes = image.copy()\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > threshold:\n",
    "            class_name = idx_to_label[label]\n",
    "            \n",
    "            # Skip ignored labels\n",
    "            if class_name in ignore_labels:\n",
    "                continue\n",
    "            \n",
    "            # Convert box coordinates to integers\n",
    "            box = box.astype(int)\n",
    "            color = colors.get(class_name, (255, 255, 255))\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(image_with_boxes, \n",
    "                         (box[0], box[1]), \n",
    "                         (box[2], box[3]), \n",
    "                         color, \n",
    "                         2)\n",
    "            \n",
    "            # Add label and score\n",
    "            label_text = f'{class_name}: {score:.2f}'\n",
    "            cv2.putText(image_with_boxes, \n",
    "                       label_text, \n",
    "                       (box[0], box[1] - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.5, \n",
    "                       color, \n",
    "                       2)\n",
    "    \n",
    "    return image_with_boxes\n",
    "\n",
    "def test_single_image(model_path, image_path, label_to_idx, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Test the model on a single image\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create idx to label mapping\n",
    "    idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(model_path, len(label_to_idx), device)\n",
    "    \n",
    "    # Prepare image\n",
    "    original_image, image_tensor = prepare_frame(cv2.imread(image_path))  # Use cv2.imread here\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor.to(device)])\n",
    "    \n",
    "    # Get predictions\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "    labels = prediction[0]['labels'].cpu().numpy()\n",
    "    scores = prediction[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Draw predictions on image, ignoring specified labels\n",
    "    result_image = draw_predictions(original_image, \n",
    "                                  boxes, \n",
    "                                  labels, \n",
    "                                  scores, \n",
    "                                  idx_to_label, \n",
    "                                  confidence_threshold)\n",
    "    \n",
    "    # Convert back to BGR for OpenCV display/save\n",
    "    result_image = cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Save the result\n",
    "    output_path = 'prediction_result.jpg'\n",
    "    cv2.imwrite(output_path, result_image)\n",
    "    print(f\"Result saved to: {output_path}\")\n",
    "    \n",
    "    return boxes, labels, scores\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = '/Users/ujjwalbhatta/Downloads/ComputerVision-RCNN/final_rcnn_model.pth'  \n",
    "    video_path = '/Users/ujjwalbhatta/Downloads/ComputerVision-RCNN/VID20241108114856.mp4'\n",
    "    \n",
    "    label_to_idx = {\n",
    "        \"Car\": 1,\n",
    "        \"Pedestrian\": 2,\n",
    "        \"Van\": 3,\n",
    "        \"Cyclist\": 4,\n",
    "        \"Truck\": 5,\n",
    "        \"Misc\": 6,\n",
    "        \"Tram\": 7,\n",
    "        \"Person_Sitting\": 8,\n",
    "        \"DontCare\": 9,\n",
    "        \"Unknown\": 10\n",
    "    }\n",
    "    \n",
    "    # boxes, labels, scores = test_single_image(model_path, image_path, label_to_idx)\n",
    "    process_video(model_path, video_path, label_to_idx, confidence_threshold=0.5, output_path='prediction_result2.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def single_txt_to_coco(txt_file, output_json, label_to_idx):\n",
    "    \"\"\"\n",
    "    Convert a single annotation TXT file to COCO format.\n",
    "    \"\"\"\n",
    "    coco_format = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "\n",
    "    annotation_id = 0\n",
    "    image_id_map = {}\n",
    "    image_id = 0\n",
    "\n",
    "    # Define categories\n",
    "    for label, idx in label_to_idx.items():\n",
    "        coco_format[\"categories\"].append({\n",
    "            \"id\": idx,\n",
    "            \"name\": label\n",
    "        })\n",
    "\n",
    "    # Read single TXT file\n",
    "    with open(txt_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(',')\n",
    "        image_name = parts[0]\n",
    "        x_min, y_min, x_max, y_max = map(float, parts[1:5])\n",
    "        class_name = parts[5]\n",
    "\n",
    "        if image_name not in image_id_map:\n",
    "            image_id_map[image_name] = image_id\n",
    "            coco_format[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": image_name,\n",
    "                \"width\": 1242,  # Replace with actual image width\n",
    "                \"height\": 375   # Replace with actual image height\n",
    "            })\n",
    "            image_id += 1\n",
    "\n",
    "        if class_name in label_to_idx:\n",
    "            coco_format[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id_map[image_name],\n",
    "                \"category_id\": label_to_idx[class_name],\n",
    "                \"bbox\": [x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                \"area\": (x_max - x_min) * (y_max - y_min),\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    # Save COCO JSON\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(coco_format, f, indent=4)\n",
    "\n",
    "    print(f\"COCO annotations saved to {output_json}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "txt_file = \"/Users/ujjwalbhatta/Downloads/ComputerVision-RCNN/test_annotation.txt\"\n",
    "output_json = \"/Users/ujjwalbhatta/Downloads/ComputerVision-RCNN/coco_annotations.json\"\n",
    "label_to_idx = {\n",
    "    \"Car\": 1,\n",
    "    \"Pedestrian\": 2,\n",
    "    \"Van\": 3,\n",
    "    \"Cyclist\": 4,\n",
    "    \"Truck\": 5,\n",
    "    \"Misc\": 6,\n",
    "    \"Tram\": 7,\n",
    "    \"Person_Sitting\": 8,\n",
    "    \"DontCare\": 9,\n",
    "    \"Unknown\": 10\n",
    "}\n",
    "single_txt_to_coco(txt_file, output_json, label_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def evaluate_coco(model_path, annotation_path):\n",
    "    # Initialize model architecture first\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=None)\n",
    "    num_classes = 10  # Your number of classes (Car, Pedestrian, etc.)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n",
    "    \n",
    "    # Now load the weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Rest of your evaluation code remains the same...\n",
    "    # Load COCO annotations\n",
    "    gt_coco = COCO(annotation_path)\n",
    "    \n",
    "    predictions = []\n",
    "    for img_id in gt_coco.getImgIds():\n",
    "        img_info = gt_coco.loadImgs(img_id)[0]\n",
    "        img_path = img_info['file_name']\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_tensor = F.to_tensor(image)\n",
    "        image_tensor = F.normalize(image_tensor, \n",
    "                                 mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model([image_tensor.to(device)])\n",
    "        \n",
    "        boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "        scores = prediction[0]['scores'].cpu().numpy()\n",
    "        labels = prediction[0]['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            predictions.append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': [float(box[0]), float(box[1]), \n",
    "                        float(box[2] - box[0]), float(box[3] - box[1])],\n",
    "                'score': float(score)\n",
    "            })\n",
    "    \n",
    "    pred_coco = gt_coco.loadRes(predictions)\n",
    "    cocoEval = COCOeval(gt_coco, pred_coco, 'bbox')\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "    \n",
    "    return {\n",
    "        'AP@0.5': cocoEval.stats[1],\n",
    "        'AP@0.75': cocoEval.stats[2],\n",
    "        'AP@0.5:0.95': cocoEval.stats[0],\n",
    "        'AP_small': cocoEval.stats[3],\n",
    "        'AP_medium': cocoEval.stats[4],\n",
    "        'AP_large': cocoEval.stats[5],\n",
    "        'AR_max1': cocoEval.stats[6],\n",
    "        'AR_max10': cocoEval.stats[7],\n",
    "        'AR_max100': cocoEval.stats[8]\n",
    "    }\n",
    "\n",
    "# Use it\n",
    "metrics = evaluate_coco('best_model.pth', '/Users/ujjwalbhatta/Downloads/ComputerVision-RCNN/coco_annotations.json')\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDetailed Evaluation Results:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_coco_metrics_tables():\n",
    "    # Create main metrics table\n",
    "    main_metrics = {\n",
    "        'Metric': [\n",
    "            'AP@[IoU=0.50:0.95, area=all]',\n",
    "            'AP@[IoU=0.50, area=all]',\n",
    "            'AP@[IoU=0.75, area=all]'\n",
    "        ],\n",
    "        'Value': [0.582, 0.806, 0.653]\n",
    "    }\n",
    "    \n",
    "    # Create area-specific AP table\n",
    "    ap_area = {\n",
    "        'Area': ['Small', 'Medium', 'Large'],\n",
    "        'AP': [0.551, 0.588, 0.600]\n",
    "    }\n",
    "    \n",
    "    # Create recall table\n",
    "    recall_metrics = {\n",
    "        'Condition': [\n",
    "            'AR@[IoU=0.50:0.95, maxDets=1]',\n",
    "            'AR@[IoU=0.50:0.95, maxDets=10]',\n",
    "            'AR@[IoU=0.50:0.95, maxDets=100]'\n",
    "        ],\n",
    "        'Value': [0.420, 0.648, 0.652]\n",
    "    }\n",
    "    \n",
    "    # Create area-specific AR table\n",
    "    ar_area = {\n",
    "        'Area': ['Small', 'Medium', 'Large'],\n",
    "        'AR': [0.611, 0.654, 0.683]\n",
    "    }\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Main AP metrics\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='Metric', y='Value', data=pd.DataFrame(main_metrics))\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Main AP Metrics')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 2. AP by area\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='Area', y='AP', data=pd.DataFrame(ap_area))\n",
    "    plt.title('Average Precision by Area')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 3. Recall metrics\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='Condition', y='Value', data=pd.DataFrame(recall_metrics))\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Recall Metrics')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 4. AR by area\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(x='Area', y='AR', data=pd.DataFrame(ar_area))\n",
    "    plt.title('Average Recall by Area')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('coco_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create LaTeX table\n",
    "    latex_table = \"\"\"\n",
    "\\\\begin{table}[h]\n",
    "\\\\centering\n",
    "\\\\begin{tabular}{lc}\n",
    "\\\\hline\n",
    "\\\\textbf{Metric} & \\\\textbf{Value} \\\\\\\\\n",
    "\\\\hline\n",
    "AP @ IoU=0.50:0.95 (all) & 0.582 \\\\\\\\\n",
    "AP @ IoU=0.50 (all) & 0.806 \\\\\\\\\n",
    "AP @ IoU=0.75 (all) & 0.653 \\\\\\\\\n",
    "\\\\hline\n",
    "AP (small objects) & 0.551 \\\\\\\\\n",
    "AP (medium objects) & 0.588 \\\\\\\\\n",
    "AP (large objects) & 0.600 \\\\\\\\\n",
    "\\\\hline\n",
    "AR @ maxDets=1 & 0.420 \\\\\\\\\n",
    "AR @ maxDets=10 & 0.648 \\\\\\\\\n",
    "AR @ maxDets=100 & 0.652 \\\\\\\\\n",
    "\\\\hline\n",
    "AR (small objects) & 0.611 \\\\\\\\\n",
    "AR (medium objects) & 0.654 \\\\\\\\\n",
    "AR (large objects) & 0.683 \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\end{tabular}\n",
    "\\\\caption{Detection performance using COCO metrics.}\n",
    "\\\\label{tab:coco_metrics}\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    with open('coco_metrics_table.tex', 'w') as f:\n",
    "        f.write(latex_table)\n",
    "\n",
    "create_coco_metrics_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
